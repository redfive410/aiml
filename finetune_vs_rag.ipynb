{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/redfive410/aiml/blob/master/finetune_vs_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# finetune-vs-rag\n",
        "Ref: https://github.com/dylanjcastillo/rag-vs-finetune\n",
        "\n",
        "Setup\n",
        "1. Create .env file\n",
        "2. Add OPENAI_API_KEY=\n",
        "3. Upload .env file (on Mac use ctrl-shift-. to see .env file)\n",
        "4. Run pip commands\n"
      ],
      "metadata": {
        "id": "p2KjTGCAZctg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "!pip install pandas\n",
        "!pip install langchain\n",
        "!pip install tiktoken\n",
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "7rvmAeQvWg66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMJ-1Yk1VuBb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbJFmn_NVuBc"
      },
      "source": [
        "## Create JSON file with population data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW8joHhEVuBd"
      },
      "outputs": [],
      "source": [
        "scenarios = [\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You're a helpful assistant\"},\n",
        "            {\"role\": \"user\", \"content\": f\"What is the population of City_{i}?\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"City_{i} has about {random.randint(100000, 10000000)} inhabitants.\"}\n",
        "        ]\n",
        "    } for i in range(1, 101)\n",
        "]\n",
        "\n",
        "jsonl_data = \"\\n\".join(json.dumps(scenario) for scenario in scenarios)\n",
        "\n",
        "with open(\"cities_population.jsonl\", \"w\") as f:\n",
        "    f.write(jsonl_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqC0JipfVuBd"
      },
      "source": [
        "## Start client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeNaanBcVuBe"
      },
      "outputs": [],
      "source": [
        "client = OpenAI()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V648ayH1VuBe"
      },
      "source": [
        "## Upload file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0nwQBGLVuBe"
      },
      "outputs": [],
      "source": [
        "client.files.create(\n",
        "  file=open(\"cities_population.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jat1ThQWVuBf"
      },
      "source": [
        "## Start fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAIC04XGVuBf"
      },
      "outputs": [],
      "source": [
        "FILE_ID=\"\"\n",
        "client.fine_tuning.jobs.create(\n",
        "  training_file=FILE_ID,\n",
        "  model=\"gpt-3.5-turbo\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZJ1VLfVVuBg"
      },
      "source": [
        "## Test fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl2_uSSkVuBg"
      },
      "outputs": [],
      "source": [
        "MODEL_ID=\"\"\n",
        "response = client.chat.completions.create(\n",
        "  model=MODEL_ID,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the population of City_1?\"},\n",
        "  ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjHdG_ZlVuBh"
      },
      "outputs": [],
      "source": [
        "message_content = response.choices[0].message.content\n",
        "message_content\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import chromadb\n",
        "\n",
        "chroma = chromadb.Client()\n",
        "try:\n",
        "    chroma.delete_collection(\"rag\")\n",
        "except:\n",
        "    pass\n",
        "chroma_collection = chroma.create_collection(\"rag\")\n",
        "\n",
        "with open(\"cities_population.jsonl\", \"r\") as f:\n",
        "    jsonl_data = f.read()\n",
        "\n",
        "data = []\n",
        "for row in jsonl_data.split(\"\\n\"):\n",
        "    data.append(json.loads(row)[\"messages\"])\n",
        "\n",
        "assistant_responses = []\n",
        "ids = []\n",
        "for i, message in enumerate(data):\n",
        "    assistant_responses.append(message[2][\"content\"])\n",
        "    ids.append(str(i))\n",
        "\n",
        "chroma_collection.add(\n",
        "    documents=assistant_responses,\n",
        "    ids=ids,\n",
        ")"
      ],
      "metadata": {
        "id": "3CW6MunAX0nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from chromadb import QueryResult\n",
        "\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "MODEL_ID = \"\"\n",
        "openai_client = OpenAI()\n",
        "\n",
        "def get_prompt(question: str, documents: QueryResult):\n",
        "    context = \"\"\n",
        "    for i, (_, document) in enumerate(\n",
        "        zip(documents[\"metadatas\"][0], documents[\"documents\"][0])\n",
        "    ):\n",
        "        context += f\"[{i}]: \\n\" + document + \"\\n\\n\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Answer the following QUESTION based on the CONTEXT only. Only answer from the Context. If you don't know the answer, say 'I don't know'.\\n\\nQUESTION: {question}\\n\\nCONTEXT: {context}\\n\\nANSWER:\"\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_fixed(1))\n",
        "def get_completion(messages, model):\n",
        "    return openai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        max_tokens=400,\n",
        "        temperature=0.0,\n",
        "        seed=42,\n",
        "        timeout=5\n",
        "    )\n",
        "\n",
        "\n",
        "def get_rag_answer(question: str, context: str, model: str):\n",
        "    messages = get_prompt(question, context)\n",
        "    response = get_completion(messages, model)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def get_finetuned_answer(question: str):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Answer this question to the best of your ability. If you don't know the answer, say 'I don't know'.\\n\\nQUESTION: {question}\\n\\nANSWER:\"\"\"\n",
        "        },\n",
        "    ]\n",
        "    response = get_completion(messages, MODEL_ID)\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "GbV4-nh-YGWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparisons = []\n",
        "for i, row in enumerate(data):\n",
        "    question = row[1][\"content\"]\n",
        "    documents = chroma_collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=5\n",
        "    )\n",
        "    response_comparison = {\n",
        "        \"response_model_ft\": get_finetuned_answer(question),\n",
        "        \"response_model_rag\": get_rag_answer(question, documents, \"gpt-3.5-turbo-1106\"),\n",
        "        \"answer\": row[2][\"content\"],\n",
        "        \"question\": question,\n",
        "    }\n",
        "    print(i, response_comparison)\n",
        "    comparisons.append(response_comparison)\n"
      ],
      "metadata": {
        "id": "WGKE6-XEYHu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_number(text):\n",
        "    matches = re.findall(r\"\\s(\\d+)\", text)\n",
        "    if len(matches) == 0:\n",
        "        return None\n",
        "    else:\n",
        "        return int(matches[0])\n",
        "\n",
        "correct_ft = 0\n",
        "correct_rag = 0\n",
        "for comparison in comparisons:\n",
        "    ft_number = extract_number(comparison[\"response_model_ft\"])\n",
        "    rag_number = extract_number(comparison[\"response_model_rag\"])\n",
        "    answer_number = extract_number(comparison[\"answer\"])\n",
        "    print(ft_number, rag_number, answer_number)\n",
        "\n",
        "    if ft_number == answer_number:\n",
        "        correct_ft += 1\n",
        "    if rag_number == answer_number:\n",
        "        correct_rag += 1\n",
        "\n",
        "print(\"Accuracy FT:\", correct_ft / len(comparisons))\n",
        "print(\"Accuracy RAG:\", correct_rag / len(comparisons))\n"
      ],
      "metadata": {
        "id": "67g_2HEhYr-h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}